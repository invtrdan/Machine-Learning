{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "m9YumbH4cZ0R",
        "2bd96NyEboqF"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/invtrdan/Machine-Learning/blob/main/Neural_Network_Training_(XOR%2C_Spirals)%2C_Model_Diagrams%2C_Decision_Boundaries.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imcNmFXhPdCh"
      },
      "source": [
        "# Import our standard libraries.\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns  # for nicer plots\n",
        "sns.set(style='darkgrid')  # default style\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9YumbH4cZ0R"
      },
      "source": [
        "---\n",
        "## Helper function(s)\n",
        "\n",
        "* `draw_neural_net(...)`: Draw a neural network diagram\n",
        "* `plot_decision_boundary(...)`: Plot a model's learned decision boundary, along with the labeled data points\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZMAvH2OU2LA"
      },
      "source": [
        "def draw_neural_net(ax, layer_sizes, coefs_, intercepts_,\n",
        "                    left=0.1, right=.9, bottom=0.1, top=.9):\n",
        "    '''\n",
        "    Draw a neural network cartoon using matplotilb.\n",
        "    Adapted: https://gist.github.com/craffel/2d727968c3aaebd10359\n",
        "\n",
        "    Args:\n",
        "        - ax : matplotlib.axes.AxesSubplot\n",
        "            The axes on which to plot the cartoon (get e.g. by plt.gca())\n",
        "        - layer_sizes : list of int\n",
        "            List of layer sizes, including input and output dimensionality\n",
        "    '''\n",
        "    n_layers = len(layer_sizes)\n",
        "    v_spacing = (top - bottom)/float(max(layer_sizes))\n",
        "    h_spacing = (right - left)/float(len(layer_sizes) - 1)\n",
        "\n",
        "    # Input-Arrows\n",
        "    layer_top_0 = v_spacing*(layer_sizes[0] - 1)/2. + (top + bottom)/2.\n",
        "    for m in range(layer_sizes[0]):\n",
        "        plt.arrow(left-0.18, layer_top_0 - m*v_spacing, 0.12, 0,  lw=1, head_width=0.01, head_length=0.02)\n",
        "\n",
        "    # Nodes\n",
        "    for n, layer_size in enumerate(layer_sizes):\n",
        "        layer_top = v_spacing*(layer_size - 1)/2. + (top + bottom)/2.\n",
        "        for m in range(layer_size):\n",
        "            circle = plt.Circle((n*h_spacing + left, layer_top - m*v_spacing), v_spacing/8.,\n",
        "                                color='w', ec='k', zorder=4)\n",
        "            if n == 0:\n",
        "                plt.text(left-0.125, layer_top - m*v_spacing, r'$X_{'+str(m+1)+'}$', fontsize=15)\n",
        "            elif (n_layers == 3) & (n == 1):\n",
        "                plt.text(n*h_spacing + left+0.00, layer_top - m*v_spacing+ (v_spacing/8.+0.01*v_spacing), r'$H_{'+str(m+1)+'}$', fontsize=15)\n",
        "            elif n == n_layers -1:\n",
        "                plt.text(n*h_spacing + left+0.10, layer_top - m*v_spacing, r'$y_{'+str(m+1)+'}$', fontsize=15)\n",
        "            ax.add_artist(circle)\n",
        "\n",
        "    # Bias-Nodes\n",
        "    for n, layer_size in enumerate(layer_sizes):\n",
        "        if n < n_layers -1:\n",
        "            x_bias = (n+0.5)*h_spacing + left\n",
        "            y_bias = top + 0.005\n",
        "            circle = plt.Circle((x_bias, y_bias), v_spacing/8., color='w', ec='k', zorder=4)\n",
        "            plt.text(x_bias-(v_spacing/8.+0.10*v_spacing-0.01), y_bias, r'$1$', fontsize=15)\n",
        "            ax.add_artist(circle)\n",
        "\n",
        "    # Edges\n",
        "    # Edges between nodes\n",
        "    for n, (layer_size_a, layer_size_b) in enumerate(zip(layer_sizes[:-1], layer_sizes[1:])):\n",
        "        layer_top_a = v_spacing*(layer_size_a - 1)/2. + (top + bottom)/2.\n",
        "        layer_top_b = v_spacing*(layer_size_b - 1)/2. + (top + bottom)/2.\n",
        "        for m in range(layer_size_a):\n",
        "            for o in range(layer_size_b):\n",
        "                line = plt.Line2D([n*h_spacing + left, (n + 1)*h_spacing + left],\n",
        "                                  [layer_top_a - m*v_spacing, layer_top_b - o*v_spacing], c='k')\n",
        "                ax.add_artist(line)\n",
        "                xm = (n*h_spacing + left)\n",
        "                xo = ((n + 1)*h_spacing + left)\n",
        "                ym = (layer_top_a - m*v_spacing)\n",
        "                yo = (layer_top_b - o*v_spacing)\n",
        "                rot_mo_rad = np.arctan((yo-ym)/(xo-xm))\n",
        "                rot_mo_deg = rot_mo_rad*180./np.pi\n",
        "                xm1 = xm + (v_spacing/8.+0.05)*np.cos(rot_mo_rad)\n",
        "                if n == 0:\n",
        "                    if yo > ym:\n",
        "                        ym1 = ym + (v_spacing/8.+0.08)*np.sin(rot_mo_rad)\n",
        "                    else:\n",
        "                        ym1 = ym + (v_spacing/8.+0.1)*np.sin(rot_mo_rad) + .01\n",
        "                else:\n",
        "                    if yo > ym:\n",
        "                        ym1 = ym + (v_spacing/8.+0.08)*np.sin(rot_mo_rad)\n",
        "                    else:\n",
        "                        ym1 = ym + (v_spacing/8.+0.08)*np.sin(rot_mo_rad)\n",
        "                plt.text(xm1, ym1,\n",
        "                         str(round(coefs_[n][m, o],2)),\n",
        "                         rotation = rot_mo_deg,\n",
        "                         fontsize = 10)\n",
        "\n",
        "    # Edges between bias and nodes\n",
        "    for n, (layer_size_a, layer_size_b) in enumerate(zip(layer_sizes[:-1], layer_sizes[1:])):\n",
        "        if n < n_layers-1:\n",
        "            layer_top_a = v_spacing*(layer_size_a - 1)/2. + (top + bottom)/2.\n",
        "            layer_top_b = v_spacing*(layer_size_b - 1)/2. + (top + bottom)/2.\n",
        "        x_bias = (n+0.5)*h_spacing + left\n",
        "        y_bias = top + 0.005\n",
        "        for o in range(layer_size_b):\n",
        "            line = plt.Line2D([x_bias, (n + 1)*h_spacing + left],\n",
        "                          [y_bias, layer_top_b - o*v_spacing], c='k')\n",
        "            ax.add_artist(line)\n",
        "            xo = ((n + 1)*h_spacing + left)\n",
        "            yo = (layer_top_b - o*v_spacing)\n",
        "            rot_bo_rad = np.arctan((yo-y_bias)/(xo-x_bias))\n",
        "            rot_bo_deg = rot_bo_rad*180./np.pi\n",
        "            xo2 = xo - (v_spacing/8.+0.01)*np.cos(rot_bo_rad)\n",
        "            yo2 = yo - (v_spacing/8.+0.01)*np.sin(rot_bo_rad)\n",
        "            xo1 = xo2 -0.08 * np.cos(rot_bo_rad)\n",
        "            yo1 = yo2 -0.05 * np.sin(rot_bo_rad)\n",
        "            plt.text(xo1, yo1,\n",
        "                 str(round(intercepts_[n][o],2)),\n",
        "                 rotation = rot_bo_deg,\n",
        "                 fontsize = 10)\n",
        "\n",
        "    # Output-Arrows\n",
        "    layer_top_0 = v_spacing*(layer_sizes[-1] - 1)/2. + (top + bottom)/2.\n",
        "    for m in range(layer_sizes[-1]):\n",
        "        plt.arrow(right+0.015, layer_top_0 - m*v_spacing, 0.16*h_spacing, 0,  lw =1, head_width=0.01, head_length=0.02)\n",
        "\n",
        "    ax.set_xlim([0, 1])\n",
        "    ax.set_ylim([0, 1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nymKlPTLaNev"
      },
      "source": [
        "def plot_decision_boundary(X, Y, model, steps=100, size=3, cmap='bwr'):\n",
        "    \"\"\"\n",
        "    Function to plot the decision boundary and data points of a model.\n",
        "    Data points are colored based on their actual label.\n",
        "    \"\"\"\n",
        "    cmap = plt.get_cmap(cmap)\n",
        "\n",
        "    # Define region of interest by data limits\n",
        "    xmin, xmax = X[:,0].min() - 1, X[:,0].max() + 1\n",
        "    ymin, ymax = X[:,1].min() - 1, X[:,1].max() + 1\n",
        "    x_span = np.linspace(xmin, xmax, steps)\n",
        "    y_span = np.linspace(ymin, ymax, steps)\n",
        "    xx, yy = np.meshgrid(x_span, y_span)\n",
        "\n",
        "    # Make predictions across region of interest\n",
        "    labels = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "\n",
        "    # Plot decision boundary in region of interest\n",
        "    z = labels.reshape(xx.shape)\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.contourf(xx, yy, z, cmap=cmap, alpha=0.3)\n",
        "\n",
        "    # Get predicted labels on training data and plot\n",
        "    train_labels = model.predict(X)\n",
        "    ax.scatter(X[:,0], X[:,1], c=Y, cmap=cmap, s=size)\n",
        "\n",
        "    return fig, ax"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1I7jrHHdBdL"
      },
      "source": [
        "---\n",
        "## The XOR Problem\n",
        "\n",
        "We'll start by generating data to replicate the XOR logical operator. As a reminder, XOR(x1, x2) = 1 if *exactly* one of inputs x1 or x2 is 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AafvM2hh9OS8"
      },
      "source": [
        "# Replicate the XOR operator's behavior.\n",
        "X = np.array([\n",
        "    [0, 0],\n",
        "    [0, 1],\n",
        "    [1, 0],\n",
        "    [1, 1]\n",
        "    ])\n",
        "\n",
        "Y = np.array([\n",
        "    0,\n",
        "    1,\n",
        "    1,\n",
        "    0\n",
        "    ])\n",
        "\n",
        "# Plot XOR data.\n",
        "pos_X = X[Y==1]\n",
        "neg_X = X[Y==0]\n",
        "plt.scatter(x=pos_X[:,0], y=pos_X[:,1], marker='+', c='r', s=1000, linewidth=4)\n",
        "plt.scatter(x=neg_X[:,0], y=neg_X[:,1], marker='_', c='k', s=1000, linewidth=4)\n",
        "plt.xlim(-0.15, 1.15)\n",
        "plt.ylim(-0.15, 1.15)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmWkXbucdJY5"
      },
      "source": [
        "### Linear Models\n",
        "\n",
        "The XOR dataset is *not* linearly separable. We cannot draw a single linear decision boundary to separate the {-} and {+} data points.\n",
        "\n",
        "Here, we'll confirm that a linear model will perform poorly when trained to solve XOR."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwRbcIIj9hjU"
      },
      "source": [
        "def build_linear_xor_model():\n",
        "  tf.keras.backend.clear_session()\n",
        "  tf.keras.utils.set_random_seed(0)\n",
        "\n",
        "  model = tf.keras.Sequential()\n",
        "\n",
        "  # Set input shape in advance\n",
        "  model.add(tf.keras.Input(shape=(2,), name='Input'))\n",
        "  # Only one layer (i.e., a binary classifier)\n",
        "  model.add(tf.keras.layers.Dense(units=1, activation='sigmoid', name='Output'))\n",
        "  model.compile(loss='binary_crossentropy',\n",
        "                optimizer=tf.keras.optimizers.SGD(learning_rate=1))\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKkRMx_797nd"
      },
      "source": [
        "# Build the model and show a summary.\n",
        "xor_model = build_linear_xor_model()\n",
        "print(xor_model.summary())\n",
        "\n",
        "# Train a model and show loss\n",
        "history = xor_model.fit(\n",
        "    x=X,\n",
        "    y=Y,\n",
        "    epochs=100,\n",
        "    batch_size=4,\n",
        "    verbose=0)\n",
        "losses = history.history['loss']\n",
        "plt.plot(losses)\n",
        "plt.xlabel('Training Step')\n",
        "plt.ylabel('Cross-Entropy Loss')\n",
        "plt.show()\n",
        "\n",
        "# Show data and predictions\n",
        "preds = xor_model.predict(X)\n",
        "for i in range(len(Y)):\n",
        "  (x1,x2) = X[i]\n",
        "  y = Y[i]\n",
        "  y_hat = preds[i]\n",
        "  print(\"x [%d %d]  y [%d]  ŷ [%.4f]\" %(x1, x2, y, y_hat))\n",
        "\n",
        "# Show learned model\n",
        "w, b = xor_model.layers[0].get_weights()\n",
        "print('f(x) = sigmoid(%.4f + %.4f*x1 + %.4f*x2)' %(b[0], w[0], w[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sr-TuwX-d38_"
      },
      "source": [
        "fig = plt.figure(figsize=(5, 5))\n",
        "ax = fig.gca()\n",
        "ax.axis('off')\n",
        "ax.set_xlim([0, 1])\n",
        "ax.set_ylim([0, 1])\n",
        "draw_neural_net(ax, [2, 1], {0:w}, {0:b})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The cells above show that:\n",
        "* The model weights in the diagram are all 0, so the *score* assigned to any input is always 0.\n",
        "* $\\hat{y}$ = z(0) = 0.5 for every datapoint. That means our model classifies every input as being equally likely to belong to either class. In other words, *our model is performing no better than guessing the label at random*.\n"
      ],
      "metadata": {
        "id": "_D_x60IhmhOa"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zymzU-OmdRSi"
      },
      "source": [
        "### Neural Network Models\n",
        "\n",
        "Now, we'll build a neural network with hidden, intermediate layers as discussed during lecture. In addition to adding more layers, we will also introduce \"nonlinearities\" called \"activation functions\" inbetween the layers. You've already seen some of these, such as the sigmoid function from binary classification.\n",
        "\n",
        "We'll see that this network will be able to solve XOR and achieve better performance than the linear model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fWTqjdiBmWT"
      },
      "source": [
        "def build_ffnn_xor_model(hidden_layers=[]):\n",
        "  tf.keras.backend.clear_session()\n",
        "  tf.keras.utils.set_random_seed(1)\n",
        "\n",
        "  model = tf.keras.Sequential()\n",
        "\n",
        "  # Set input shape in advance\n",
        "  model.add(tf.keras.Input(shape=(2,), name='Input'))\n",
        "\n",
        "  # Add intermediate layers to our model\n",
        "  #\n",
        "  # Example: if `hidden_layers`=[2, 4, 2],\n",
        "  # this create 3 intermediate layers with 2, 4, and 2 units, respectively.\n",
        "  for num_nodes in hidden_layers:\n",
        "    model.add(tf.keras.layers.Dense(units=num_nodes, # Number of nodes in the layer\n",
        "                                    activation='relu', # Activation function -ve values to 0\n",
        "                                    name='Hidden'))\n",
        "\n",
        "  # Set output to produce a single value using `sigmoid` as our final activation function\n",
        "  model.add(tf.keras.layers.Dense(units=1, activation='sigmoid', name='Output'))\n",
        "\n",
        "  model.compile(loss='binary_crossentropy',\n",
        "                optimizer=tf.keras.optimizers.SGD(learning_rate=1))\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_2gV1SXB-K3"
      },
      "source": [
        "# Build the model and show a summary.\n",
        "xor_model = build_ffnn_xor_model(hidden_layers=[2])\n",
        "print(xor_model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a model and plot the cross-entropy loss.\n",
        "history = xor_model.fit(\n",
        "    x = X,\n",
        "    y = Y,\n",
        "    epochs=2000,\n",
        "    batch_size=4,\n",
        "    verbose=0)\n",
        "losses = history.history['loss']\n",
        "plt.plot(losses)\n",
        "plt.xlabel('Training Step')\n",
        "plt.ylabel('Cross-Entropy Loss')\n",
        "plt.show()\n",
        "\n",
        "# Make predictions for each datapoint.\n",
        "preds = xor_model.predict(X)\n",
        "for i in range(len(Y)):\n",
        "  (x1, x2) = X[i]\n",
        "  y = Y[i]\n",
        "  y_hat = preds[i]\n",
        "  print(\"x [%d %d]  y [%d]  ŷ [%.4f]\" %(x1, x2, y, y_hat))"
      ],
      "metadata": {
        "id": "DGV_qNxcn1lL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYuyEihTVkJu"
      },
      "source": [
        "# Get model weights.\n",
        "w0, b0 = xor_model.layers[0].get_weights()\n",
        "w1, b1 = xor_model.layers[1].get_weights()\n",
        "\n",
        "# Plot model weights.\n",
        "fig = plt.figure(figsize=(8, 8))\n",
        "ax = fig.gca()\n",
        "ax.axis('off')\n",
        "ax.set_xlim([0, 1])\n",
        "ax.set_ylim([0, 1])\n",
        "draw_neural_net(ax, [2, 2, 1], {0:w0, 1:w1}, {0:b0, 1:b1})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HYf2_JbzX4P"
      },
      "source": [
        "### Forward Pass Prediction\n",
        "\n",
        "Use the learned weights shown in the model diagram to fill in the intermediate values $h_1$ and $h_2$ and the final predictions $\\hat{y}$. Compare the predictions to the true labels $y$.\n",
        "\n",
        "Remember to use the right activation function for each node corresponding to how we designed our neural network!\n",
        "\n",
        "Hint:\n",
        "* h_1 = z(x_1 * w0[0, 0] + x_2 * w0[1, 0] + 1 * b0[0])\n",
        "* h_2 = z(x_1 * w0[0, 1] + x_2 * w0[1, 1] + 1 * b0[1])\n",
        "* y = z(h_1 * w1[0, 0] + h_2 * w1[1, 0] + 1 * b1[0])\n",
        "\n",
        "$x_1$ | $x_2$ | $h_1$ | $h_2$ | $\\hat{y}$ | $y$\n",
        "-|-|-|-|-|-\n",
        "0|0| TODO | TODO | TODO | TODO\n",
        "0|1| TODO | TODO | TODO | TODO\n",
        "1|0| TODO | TODO | TODO | TODO\n",
        "1|1| TODO | TODO | TODO | TODO"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(x):\n",
        "  return max(0, x)\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "for x1 in [0, 1]:\n",
        "  for x2 in [0, 1]:\n",
        "    print(\"x1={0}, x2={1}:\".format(x1, x2))\n",
        "    h1 = relu(x1 * w0[0, 0] + x2 * w0[1, 0] + b0[0])\n",
        "    h2 = relu(x1 * w0[0, 1] + x2 * w0[1, 1] + b0[1])\n",
        "    print(\"h1:\", h1)\n",
        "    print(\"h2:\", h2)\n",
        "    y_pred = sigmoid(h1 * w1[0, 0] + h2 * w1[1, 0] + b1[0])\n",
        "    print(\"y_pred:\", y_pred)"
      ],
      "metadata": {
        "id": "Ny4RTR0v0Uzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmOlUNx4bcL9"
      },
      "source": [
        "### Decision Boundary\n",
        "\n",
        "Use the helper function to draw the decision boundary learned by the XOR model.\n",
        "\n",
        "As a reminder, a decision boundary represents some line, curve, or \"hyperplane\" that a machine learning classifer learns to distinguish datapoints belonging to different classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKWGewOgacK_"
      },
      "source": [
        "plot_decision_boundary(X, Y, xor_model, size=30)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dljATnV-aEwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bd96NyEboqF"
      },
      "source": [
        "## The Spiral Problem\n",
        "\n",
        "Here is another interesting dataset that is also not linearly separable, and as a result cannot be solved by a linear model. We'll talk about this more during the upcoming lectures, so this is just a sneak peek."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFP-qC9LbwJQ"
      },
      "source": [
        "def generate_spiral_data(num=1000):\n",
        "  np.random.seed(1)\n",
        "  X = []\n",
        "\n",
        "  # The delta values correspond to different offsets for the 2 classes.\n",
        "  for delta in [0, np.pi]:\n",
        "    for i in range(num):\n",
        "      r = 1.0 * i / num * 5\n",
        "      t = 1.75 * i / num * 2 * np.pi + delta\n",
        "      x0 = r * np.sin(t) + (2 * np.random.rand() - 1) * 0.1\n",
        "      x1 = r * np.cos(t) + (2 * np.random.rand() - 1) * 0.1\n",
        "      X.append([x0, x1])\n",
        "\n",
        "  X = np.array(X)\n",
        "  Y = np.concatenate([np.zeros(num), np.ones(num)]).astype(int)\n",
        "\n",
        "  shuf_idx = np.random.permutation(len(Y))\n",
        "  X = X[shuf_idx]\n",
        "  Y = Y[shuf_idx]\n",
        "\n",
        "  return X, Y\n",
        "\n",
        "X, Y = generate_spiral_data(1000)\n",
        "plt.scatter(X[:,0], X[:,1], c=Y, cmap='bwr', s=3)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxSGZnwfcgT-"
      },
      "source": [
        "def build_spiral_model(hidden_layer_sizes=[], seed=10):\n",
        "  tf.keras.backend.clear_session()\n",
        "  tf.keras.utils.set_random_seed(seed)\n",
        "\n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(tf.keras.Input(shape=(2,), name='Input'))\n",
        "  for hidden_layer_size in hidden_layer_sizes:\n",
        "    model.add(tf.keras.layers.Dense(units=hidden_layer_size,\n",
        "                                    activation='relu'))\n",
        "  model.add(tf.keras.layers.Dense(units=1, activation='sigmoid', name='Output'))\n",
        "  model.compile(loss='binary_crossentropy',\n",
        "                optimizer=tf.keras.optimizers.SGD(learning_rate=.05))\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgDJyzXAdyh0"
      },
      "source": [
        "### Neural Network Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VX1mjahxcuuX"
      },
      "source": [
        "layer_sizes = [8, 8, 4, 4]\n",
        "spiral_model = build_spiral_model(hidden_layer_sizes=layer_sizes)\n",
        "print(spiral_model.summary())\n",
        "\n",
        "history = []\n",
        "\n",
        "plot_decision_boundary(X, Y, spiral_model, size=3)\n",
        "plt.show()\n",
        "\n",
        "for i in range(20):\n",
        "  history = spiral_model.fit(\n",
        "      x=X, y=Y, epochs=50, batch_size=128, verbose=0)\n",
        "\n",
        "  plot_decision_boundary(X, Y, spiral_model, size=3)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "losses = history.history['loss']\n",
        "plt.plot(losses)\n",
        "plt.xlabel('Training Step')\n",
        "plt.ylabel('Cross-Entropy Loss')\n",
        "label = '{:.2f}'.format(losses[-1])\n",
        "plt.annotate(label, (len(losses), losses[-1]))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08zmwwQqd7LM"
      },
      "source": [
        "weights_as_dict = {}\n",
        "biases_as_dict = {}\n",
        "for i in range(len(layer_sizes) + 1):\n",
        "  w, b = spiral_model.layers[i].get_weights()\n",
        "  weights_as_dict[i] = w\n",
        "  biases_as_dict[i] = b\n",
        "\n",
        "fig = plt.figure(figsize=(12, 12))\n",
        "ax = fig.gca()\n",
        "ax.axis('off')\n",
        "draw_neural_net(ax, [2]+layer_sizes+[1], weights_as_dict, biases_as_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTfdPxgSd4II"
      },
      "source": [
        "### Decision Boundary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYVJ3xMFdP-9"
      },
      "source": [
        "plot_decision_boundary(X, Y, spiral_model, size=3)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}